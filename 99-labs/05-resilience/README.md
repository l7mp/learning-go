# Cloud-native patterns: Resilience

In the course of this lab we will continue our journey to develop our toy web app, SplitDim, into a real cloud native microservice application that is stateless, scalable and resilient. After we achieved immutability at the end of the previous lab, this time we concentrate on resiliency and work towards making SplitDim resilient to certain types of failures, especially downstream network problems.

![SplitDim logo, generated by logoai.com.](/99-labs/fig/splitdim-logo.png)

The below tasks guide you in making the web app (a little bit more) resilient. The tasks are followed by tests; pass each to complete the lab.

## Table of Contents

1. [Preliminaries](#preliminaries)
2. [Retry](#retry)
3. [Graceful shutdown](#graceful-shutdown)
4. [Health checks](#health-checks)
5. [Transactions revisited](#transactions-revisited)

## Preliminaries

Recall, SplitDim is a web app that lets groups of people keep track of who owns who. The web service implements the following API endpoints:
- `GET /`: a static HTML/JS artifact that you can use to interact with the app from a browser,
- `POST /api/transfer`: register a transfer of a given amount between two users,
- `GET /api/accounts`: return the list of current balances for each registered user,
- `GET /api/clear`: return the list of transfers that would allow users to clear their debts, and
- `GET /api/reset`: reset all balances to zero.

Perhaps the most curious part of the code is our current implementation of the `Transfer` API function as shown in the simplified code below. 

```go
// Transfer represents a transaction.
type Transfer struct {
	// The debtor.
	Sender string `json:"sender"`
	// The creditor.
	Receiver string `json:"receiver"`
	// The amount transferred in the transaction.
	Amount int `json:"amount"`
}

// Transfer will process a transfer.
func (db *kvstore) Transfer(t Transfer) error {
    for {
        if err := db.setBalance(t.Sender, t.Amount); err != nil { continue }
        break
    }

    for {
        if err := db.setBalance(t.Receiver, -t.Amount); err != nil { continue }
        break
    }

    return nil
}
```

Recall, this function takes a `Transfer` API object and increases the balance of the sender by the requested amount and decreases the balance of the receiver by the same amount. Theoretically, the two operations should occur *at the same time* to complete the transfer. The `db.setBalance` function is our little helper that makes a `get` call to obtain the current versioned balance from the key-value store followed by a `put` that tries to register the new balance (with the same version) in the accounts database.

```go
func (db *kvstore) setBalance(user string, amount int) error {
    vv, err := db.Get(user)
    if err != nil { 
        return err
    }

    vv.balance += amount      // With the int-conversion removed
    vkv := clientapi.VersionedKeyValue{user, vv}

    err = db.Put(vkv)
    if err != nil {
        return err
    }

    return nil
}
```

It is completely normal for the `put` to fail; this happens, e.g., when another `splitdim` instance makes a `put` to the same account between our `get` and `put` queries.

There are two problems with the current code:
- Currently we cannot ensure the "at the same time" property: since our key-value store does not support transactions, it is certainly possible that the first `db.setBalance` operation succeeds while the second one fails, which will leave the account database in an inconsistent state. The most we can do is to try to avoid such situations as much as we can (see [later](#transactions-revisited) on transaction support).
- Perhaps less obvious, but there is another problem lurking in the code: if `db.SetBalance` fails and this failure persists, then `Transfer` falls into an infinite loop trying to update the key-value store to no avail.

It is easy to test this:
- Restore the Kubernetes configuration from the previous lab:
  ```shell
  cd 99-labs/code/kvstore
  kubectl apply -f deploy/kubernetes-statefulset.yaml
  cd 99-labs/code/splitdim
  kubectl apply -f deploy/kubernetes-kvstore.yaml
  kubectl scale deployment splitdim --replicas=1
  ```
- Make a quick test:
  ```shell
  export EXTERNAL_IP=$(kubectl get service splitdim -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
  export EXTERNAL_PORT=80
  curl -H "Content-Type: application/json" --request POST --data '{"sender":"a","receiver":"b","amount":1}' http://${EXTERNAL_IP}:${EXTERNAL_PORT}/api/transfer
  curl -H "Content-Type: application/json" --request POST --data '{"sender":"b","receiver":"c","amount":1}' http://${EXTERNAL_IP}:${EXTERNAL_PORT}/api/transfer
  curl http://${EXTERNAL_IP}:${EXTERNAL_PORT}/api/clear
  [{"sender":"c","receiver":"a","amount":1}]
  ```
- Now remove the key-value store and try to apply another transaction:
  ```shell
  kubectl delete -f deploy/kubernetes-statefulset.yaml
  curl -H "Content-Type: application/json" --request POST --data '{"sender":"a","receiver":"b","amount":1}' http://${EXTERNAL_IP}:${EXTERNAL_PORT}/api/transfer
  ```
- You should see `curl` hanging for a while just to timeout after a longish wait. Meanwhile the `splitdim` pod is going completely berserk, filling the log with the traces of desperately trying to reach the key-value store:
  ```shell
  kubectl logs $(kubectl get pods -l app=splitdim -o jsonpath='{.items[0].metadata.name}') -f
  kvstore_db.go:52: transfer: could not set balance for user "a": get: HTTP error: Post "http://kvstore.default:8081/api/get": dial tcp: lookup kvstore.default on 10.96.0.10:53: no such host
  ...
  ```

> [!TIP]
> 
> In order to avoid that the `splitdim` pod take up all your CPU (and be OOM-killed by Kubernetes), you can apply a resource limit to the `splitdim` container to keep its maximum CPU utilization at below 100 mcore (0.1 CPU). This amounts to adding the following `resource` definition to the pod template in the `splitdim` Deployment:
> ```yaml
> apiVersion: apps/v1
> kind: Deployment
> metadata:
>   name: splitdim
>   ...
> spec:
>   selector: ...
>   template:
>     metadata: ...
>     spec:
>       containers:
>       - name: splitdim
>         ...
>         resources:
>           requests:
>             cpu: "100m"
>           limits:
>             cpu: "100m"
>           ...
> ```
> 
> This will let us isolate the failing pod, without it bringing down the whole microservice application.

## Retry

Easily, trying to apply an operation to a downstream dependency in an infinite loop, like `splitdim` attempting desperately to talk to `kvstore`, will not work when the downstream dependency fails. A straightforward solution to make our web app *resilient* to such downstream failures is to control the *retry* process: say, we could resend the failing query a certain (small) number of times in the hope that one of the attempts will succeed, and give up and report a failure only after that.  Meanwhile, we have to make sure that we do not worsen the situation by, say, causing a "retry storm".

Below, we will substitute the failure-prone infinite loop with a configurable retry policy. But before that, an important fact worth remaking at this point.

> [!WARNING]
> 
> In order to be able to retry a failing API call, the API has to be **retriable** in the first place. Most APIs (especially legacy ones) do not possess this property. Do not blindly retry an API operation unless you are absolutely sure it is safe, otherwise you may cause more trouble than if you immediately failed the operation in the first place!

One property that allows safe retriability is *idempotence*; an idempotent call can be repeated as many times as we want and the resultant state would not differ.  This makes sure that even if the caller mistakenly assumes that a call failed (e.g., by failing to get an acknowledgment) and inadvertently reapplies the same operation more than once, the resultant downstream state will not change. For instance, `put` is idempotent while `db.setBalance` is not: applying it, say, twice, will increase/decrease the user's balance by twice the requested amount. Yet, we can still retry a failed `db.setBalance` call, provided that we immediately stop the retry loop as soon as we receive a positive response (i.e., 200 HTTP status).

> [!NOTE]
>
> Retriability is a much more complex topic than what we can cover here. 

In order to simplify the implementation of the retry policy, we have packaged a small library called `resilient` next to `splitdim` (see `99-labs/code/resilient`). The API exposed by this library is as follows:
```go
type Closure func() error

// Backoff allows to configure the parameters used for the random exponential backoff algorithm.
type Backoff struct {
	// The initial duration.
	Base time.Duration
	// An upper limit on the delay between retries.
	Cap time.Duration
	// The sleep time is the base plus an additional random jitter.
	Jitter float64
	// NumTrials is the number of times the function is run.
	NumTrials int
}

// WithRetry takes a Closure and a set of backoff parameters and returns a function that, when run,
// retries the Closure using random exponential backoff on failure.
func WithRetry(f Closure, wait Backoff) Closure
```

> [!TIP]
> 
> You can browse the full documentation of the `resilient` package by starting a local godoc server in the root directory of the `resililient` package with `godoc -http=:6060` and then opening [`http://localhost:6060/pkg/resilient`](http://localhost:6060/pkg/resilient) in your browser.

First, as usual, we have to make the `resilient` package available in `splitdim` (don't forget to "replace" the package search path with the local version):
```shell
go get resilient
go mod edit -replace resilient=../resilient
go mod tidy
go mod vendor
```

> [!WARNING]
> 
> Make sure you understand the use of `go mod`; from this point we will omit package imports all together.

So let's try to make `Transfer` resilient to downstream failures using the `resilient` package.  The naive solution to avoid the infinite loop would be to apply `resilient.WithRetry` with an adequate backoff policy to the `setBalance` calls in `Transfer` to obtain a function which, when called, will automatically retry itself as many times as we want. Unfortunately, this will not work: `resilient.WithRetry` only accepts functions of type `func() error`, while the type of `setBalance` is `func (string, int) error`. There is a simple way to overcome such issues: define a new closure called `setBalanceForUser` that returns a specialized function that sets the balance for *one particular user* with *one particular amount*:

```go
func (db *kvstore) setBalanceForUser(user string, amount int) resilient.Closure {
    return func() error { return db.setBalance(user, amount) }
}
```

Observe that the function returned from `setBalanceForUser` now conforms to the `resilient.Closure` type, so we can now apply `resilient.WithRetry` to this function.

> [!NOTE]
> 
> Understanding and using closures is tricky. We recommend you to spend some time with the above and try to understand what's going on: closures are en extremely powerful tool once you learn how to use them.

So below is a sequence of steps that will make sure `Transfer` survives key-value store failures:

1. Create a `defaultBackoff` retry policy that will allow at most 6 retries, using the base time of 150 ms and 2 sec timeout.
2. Call `db.setBalanceForUser(t.Sender, t.Amount)` to create a `Closure` that will increase the balance of the sender by the requested amount that can now be passed to `resilient.WithRetry`.
3. Use `resilient.WithRetry` on the closure obtained in the previous step using the default backoff policy to decorate it with a retry policy.
4. Call the decorated closure returned from `resilient.WithRetry` to actually set the balance: if this fails that means that all retries have failed so we can safely return an error.
5. Now repeat the same steps for the receiver: wrap the `db.setBalanceForUser(t.Receiver, -t.Amount)` call in a new `Closure` that, when called, will decrease the balance of the receiver by the requested amount, use `resilient.WithRetry` with the default backoff policy to obtain a retrier from the new closure, and call it.
6. If this fails then we are in trouble: we have a *halfway applied transfer transaction*. The only thing we can do is to try to undo the first operation (i.e., decrease the balance of the sender by the same amount). Make sure to use more aggressive retry policy this time.
7. If this undo operation fails then *we are left with an inconsistent account database*. If this is the case, you may return an unmissable error and fail all subsequent transfers until the system operator restores the database (we won't implement that in this lab).

> [!NOTE]
> 
> Recall, you can always use the transaction log to restore the key-value store. This is why we made it *persistent* in the first place!

> [!NOTE]
> 
> Calling `resilient.WithRetry` in the `Transfer` function will create a new retrier every time we call `Transfer`. This is not a problem as the retrier decorator is stateless, but the if we were to use the circuit breaker or the rate-limiter decorators then this would not work as these decorators maintain internal state which will become unavailable once we recreate them.

You may want to update the existing `kvstore` datalayer (`splitdim/pkg/db/kvstore`) in the `splitdim` app or you can put the new code into a new subpackage (say, `splitdim/pkg/db/resilientkvstore`) as well; the choice is on you. In any way, use the usual two environment variables, `KVSTORE_MODE` and `KVSTORE_ADDR`, to select the key-value store datalayer implementation (say, `local`, `kvstore`, `resilientkvstore`, etc) and (optionally) specify the key-value store address on startup.

To actually make use of the improved code, first try a local build with a missing key-value store backend (this will make all transfers fail) and check whether a `curl` call to `/api/transfer` will fail in a controlled way (instead of falling into an infinite retry loop). Then, test with Kubernetes:

1. Enable the Istio service mesh in the cluster and apply the below *fault injection* policy that will fail roughly every third call to the `/api/put` API (make sure Istio is installed!):
   ```shell
   kubectl label namespace default istio-injection=enabled --overwrite
   kubectl apply -f - <<EOF
   apiVersion: networking.istio.io/v1alpha3
   kind: VirtualService
   metadata: { name: kvstore-500 }
   spec:
     hosts: [ kvstore ]
     http:
       # requests to the "/api/put" path will return 500 status
       - match: [ uri: { exact: "/api/put" } ]
         fault: { abort: { httpStatus: 500, percentage: { value: 33 } } }
         route: [ destination: { host: kvstore } ]
       # default route: everything that is not a "put" ("list" and "get")
       - route: [ destination: { host: kvstore } ]
   EOF
   ```

2. Rebuild the `splitdim` and the `kvstore` images.

3. Restart the `splitdim` Deployment and the key-value store.
   ```shell
   kubectl rollout restart deployment splitdim
   kubectl rollout restart statefulset kvstore
   ```

4. Wait until all pods restart and then make some simple tests with `curl` to check if everything works fine.

> ✅ **Check**
> 
> Test your Kubernetes deployment. If all goes well, you should see the output `PASS`.
> ``` sh
> export EXTERNAL_IP=$(kubectl get service splitdim -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
> export EXTERNAL_PORT=80
> go test ./... --tags=httphandler,api,localconstructor,reset,transfer,accounts,clear -v -count 1
> PASS
> ```

After the test passes, you can disable Istio service mesh to save resources.
```shell
kubectl label namespace default istio-injection=disabled --overwrite
kubectl rollout restart deployment splitdim
kubectl rollout restart statefulset kvstore
```

> [!TIP]
> 
> Feel free to add a retrier for the rest of the API calls in `splitdim`. 

## Graceful shutdown

A closer look at the code reveals another problem: what if the app dies between the both key-value store updates required to fully process a transfer complete? Say, the first call that increases the balance of the sender succeeds, but then the app dies for some reason and the second call that would decrease the balance of the receiver fails? We are again left with an inconsistent account database. What is worse, in Kubernetes such cases can happen completely legitimately; e.g., when scaling down the `splitdim` application as demand drops and so we need fewer pods, or during a software upgrade when we transition to a new version of `splitdim` and we kill the pods running the old version. Easily, we need a way to guarantee that even if the `splitdim` pod is terminated it will keep running until it finishes serving all outstanding client requests. This is called *graceful shutdown*, and it is one of the most critical resilience features a cloud native app should implement.

Recall, implementing graceful shutdown in an app means to (1) listen to `SIGTERM` signals received from Kubernetes, (2) finish processing outstanding client requests, and then (3) exit voluntarily when the application is ready to shut down. Below are a couple of pointers that will guide you in adding the required modifications to the `main` function of the `splitdim` app to implement graceful shutdown:
- Create a HTTP server object by calling, e.g., `s = &http.Server{Addr: ":8080"}`, and start it from a separate goroutine using `server.ListenAndServe()`. The goroutine should exit when the underlying network connection is closed from the main thread of the program: make sure to indicate an error to the user only if the error returned from `server.ListenAndServe` is not a "server connection closed" error (`http.ErrServerClosed`).
- Let the main thread wait until it receives a `SIGTERM` or a `SIGINT`. In Go this amounts to creating a channel that will become readable when one of the indicated signals is caught by the process:
  ```go
  sigChan := make(chan os.Signal, 1)
  signal.Notify(sigChan, syscall.SIGINT, syscall.SIGTERM)
  <-sigChan
  ```
- Call `server.Shutdown(ctx)` with a context `ctx` that will time out after a certain grace period (say, 10 minutes): this will keep the server alive until either (1) all outstanding client requests are served (e.g., all transfers are fully processed) or (2) the grace period passes. Once any of the two conditions become true, the app exits.
- Add `terminationGracePeriodSeconds: 600` to the [pod spec](https://kubernetes.io/docs/tutorials/services/pods-and-endpoint-termination-flow) in the `splitdim` Deployment manifest to tell Kubernetes that it should wait 10 minutes after sending the termination signal to the pod before actually killing it. 
  ```yaml
  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: splitdim
    labels: { app: splitdim }
  spec:
    selector: ...
    template: ...
      spec:
        terminationGracePeriodSeconds: 600
        containers:
        - name: splitdim
          image: localhost/splitdim:latest
          ...
  ```
  During this time Kubernetes will not send new HTTP requests to the pod. The 10 mins grace period should give comfortable time for the app to process all queued requests and exit by itself.

Rebuild the container image, redeploy the `splitdim` Deployment, and rerun some manual tests with curl. If all goes well, you should see no major difference from a normal test, but the code should be more robust now. You can try to scale up/down the `splitdim` Deployment app while sending it a lot of requests: if we got everything right we should never be left with an inconsistent database.

> [!TIP]
> 
> You can check the consistency of the accounts database by calling the `/api/clear`. Recall, the first thing `Clear` does is to check whether the account balances add up to zero. A halfway applied transfer will most probably leave behind state in which this condition does not hold.

> ✅ **Check**
> 
> Test your Kubernetes deployment. 
>   ``` sh
>   export EXTERNAL_IP=$(kubectl get service splitdim -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
>   export EXTERNAL_PORT=80
>   go test ./... --tags=httphandler,api,localconstructor,reset,transfer,accounts,clear -v -count 1
>   PASS
>   ```

## Health checks

The last thing we will do is to add health checking to the `splitdim` app. This makes sure Kubernetes has a way to test whether our application is up and running. For simplicity we will use the same HTTP server for serving both the main SplitDim API and health check requests: this is a common practice that simplifies configuration a lot, but may create false negatives under heavy load (i.e., when there are lots of client requests the app may start to drop health check probes). For brevity, we won't implement deep health checks and won't test downstream availability: deep health checks are mostly made irrelevant by Kubernetes selectively health-checking all downstream dependencies all by itself anyway.

Adding health-checking to a Go web service is extremely simple: (1) just add a new HTTP handler that serves GET requests on the the `/healthz` path and returns status 200 (`http.StatusOK`) and, optionally, writes `"OK"` into the response body, and then tell Kubernetes that it can use the pod's main HTTP server port (currently TCP 8080) to probe its health:
```yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: splitdim
  labels: { app: splitdim }
spec:
  selector: ...
  template: ...
    spec:
      terminationGracePeriodSeconds: 600
      containers:
      - name: splitdim
        image: localhost/splitdim:latest
        ...
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8080
          initialDelaySeconds: 3
          periodSeconds: 2
```

And this should be it. Rebuild the `splidim` image, reapply the Kubernetes manifest. Watch for the status of the `splitdim` pods: if you see lots of restarts then you misconfigured something (look for the `RESTARTS` column in the output of `kubectl get pods`). The telltale sign of a misconfigured liveness probe is Kubernetes restarting a pod every 5-10 seconds.

> ✅ **Check**
> 
> Test your Kubernetes deployment. 
>   ``` sh
>   export EXTERNAL_IP=$(kubectl get service splitdim -o jsonpath='{.status.loadBalancer.ingress[0].ip}')
>   export EXTERNAL_PORT=80
>   go test ./... --tags=httphandler,api,localconstructor,reset,transfer,accounts,clear -v -count 1
>   PASS
>   ```

## Transactions revisited

We have seen how much trouble we have to go through just to work around a major limitation of our key-value store, namely that it does not seem to support transactional updates. To be absolutely fair, this is not entirely true: in fact it supports a simplified form of transactions that would already be enough to overcome the problems we have seen above.

In particular, the key-value store serves a `/api/transaction` API endpoint that takes a list of `put` requests and performs all in a single go, while holding the database lock. The Go code signature is as follows:
```go
func transaction(opList []api.VersionedKeyValue) error
```
If any of the `put` operations in the `opList` fails then the whole transaction fails. We could have used this API from the beginning but that would have removed many of the educational lessons from this lab. Let this serve as a reminder of the importance of using a transactional databases, they serve a good purpose!

> [!TIP]
> 
> Feel free to experiment with the transactional API. Here are some ideas on possible improvements:
> - Rewrite the key-value store data layer of `splitdim` to make use of the new API. Put your transactional data-layer implementation into a new package, say, `pkg/db/transactionalkvstore`.
> - Implement transactions in the kvstore client (`kvstore/pkg/client`) to simplify the calling of the new transactional API. Let the signature added to the client interface be the following:
>   ```go
>	Transaction([]api.VersionedKeyValue) error
>   ```
> This exercise is optional, but it should give you a nice opportunity to exercise your Go skills!

<!-- Local Variables: -->
<!-- mode: markdown; coding: utf-8 -->
<!-- eval: (auto-fill-mode -1) -->
<!-- visual-line-mode: 1 -->
<!-- markdown-enable-math: t -->
<!-- End: -->
